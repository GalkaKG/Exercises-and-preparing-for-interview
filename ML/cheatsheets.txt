Разлика между Supervised, Unsupervised и Reinforcement Learning
Тип ML	Описание	Примери
Supervised Learning	Моделът се обучава на основа на данни с етикети (input-output). Целта е да се предвиди изходна стойност за нови данни.	- Кредитен риск (предсказване на "default" или "non-default").
- Прогнозиране на отлив.
Unsupervised Learning	Моделът намира скрити структури в данните, които нямат етикети (само input).	- Групиране на клиенти (segmentation).
- Намаляване на размерността (PCA).
Reinforcement Learning	Моделът се обучава чрез взаимодействие със средата и получава награди/наказания за своите действия.	- Оптимизация на кредитно портфолио.
- Търговски стратегии.


#############################################
Алгоритми за Supervised Learning (с надзор)

##### Регресия

1.Линейна регресия (Linear Regression):
Използва се за прогнозиране на непрекъснати стойности (например цена на жилище).

Какво представлява хипотезата в линейната регресия?
Хипотезата е линейна функция, която моделира връзката между входните променливи (features) и изходната променлива (target):

Как се оценява грешката (MSE/MAE)?
MSE (Mean Squared Error): Средната стойност на квадрата на грешките:
MAE (Mean Absolute Error): Средната абсолютна стойност на грешките:

Как се интерпретират коефициентите?
Коефициентите показват как промяната на съответната характеристика с единица влияе на целевата променлива, при фиксиране на останалите характеристики.



2.Логистична регресия (Logistic Regression):
Използва се за класификация (например бинарна класификация - одобрение на кредит).


Каква е функцията, използвана за класификация?
Логистичната регресия използва сигмоидна функция за прогнозиране на вероятности

Как интерпретираме изхода?
Изходът е вероятност за принадлежност към даден клас (обикновено бинарен). Ако вероятността е над 0.5, се приема клас 1, иначе клас 0.


##### Класификация

1.Decision Trees (Решаващи дървета)

Подходящи за разбираеми и лесно интерпретируеми модели.

Какво представляват критериите за разделяне?
Gini Index: Мярка за чистота на клъстерите. По-малка стойност = по-хомогенен клъстер.
Entropy (Information Gain): Мярка за намаляване на несигурността при разделяне.

Какви са предимствата и недостатъците?
Предимства: Лесни за разбиране и интерпретация, подходящи за категориални и числови данни.
Недостатъци: Могат да overfit-нат, ако са твърде дълбоки.

2.Random Forest (Случайни гори)
Съставен алгоритъм, който комбинира множество решаващи дървета.

Как намалява overfitting?
Комбинира множество независими дървета и усреднява резултатите, което намалява пренасищането с данни.

Как се определя значимостта на характеристиките?
Значимостта се изчислява въз основа на това колко пъти и колко ефективно дадена характеристика участва в разделянето на данните.

Random Forest е мощен алгоритъм за машинно обучение, който използва концепцията за ансамбли. Основните характеристики на Random Forest включват:
1. Ансамблов метод
Random Forest комбинира множество решаващи дървета (decision trees) в един ансамблов модел.
Крайният резултат се базира на:
Средна стойност на предвижданията за регресионни задачи.
Гласуване с мнозинство за класификационни задачи.
2. Случайност (Randomness)
Случайност в избора на данни:
Използва техниката Bagging (Bootstrap Aggregation), като всяко дърво се обучава на случайно избран поднабор от данните.
Случайност в избора на характеристики (features):
При всяко разделяне на възел в дървото, Random Forest избира само поднабор от характеристиките вместо всички.
3. Редуциране на Overfitting
Чрез комбиниране на много дървета и използване на случайност в обучението, Random Forest намалява риска от overfitting спрямо единичните решаващи дървета.
4. Нелинейни зависимости
Random Forest може да улавя сложни и нелинейни зависимости между характеристиките, тъй като дърветата разделят пространството на данните на сложни области.
5. Оценка на важността на характеристиките
Random Forest може да изчисли важността на характеристиките, като измерва колко често дадена характеристика се използва за разделяне на възел в дърветата.
6. Вътрешна валидирация с OOB (Out-of-Bag Error)
Моделът използва данни, които не са били избрани в случайния поднабор (out-of-bag data), за да оцени точността си, без да е нужно отделно тестово множество.
7. Устойчивост към шум и липсващи данни
Random Forest е устойчив на:
Шум в данните, защото ансамбловият подход заглушава влиянието на грешните предвиждания.
Липсващи стойности, тъй като разделянията в дърветата не зависят задължително от всички характеристики.
8. Поддръжка на висока дименсионалност
Random Forest работи добре с голям брой характеристики, като избира поднабори от тях за всяко дърво.
Random Forest може да се използва за:
Класификационни задачи (например предвиждане на класове).
Регресионни задачи (например предвиждане на числени стойности).
10. Контролируеми хиперпараметри
Random Forest има няколко важни хиперпараметри, които позволяват настройка на модела според задачата:
n_estimators: Броят на дърветата в гората.
max_depth: Максималната дълбочина на всяко дърво.
max_features: Максималният брой характеристики, които да се използват за разделяне.
min_samples_split: Минималният брой примери, необходими за разделяне на възел.
min_samples_leaf: Минималният брой примери в листо.
Недостатъци на Random Forest
По-голямо време за обучение спрямо единични дървета.
Трудна интерпретация на модела, тъй като комбинира множество дървета.
Неефективен при екстремно големи данни, когато няма достатъчно ресурси.


3.SVM (Support Vector Machines)
Support Vector Machines е мощен и популярен алгоритъм за класификация и регресия, който работи като намира оптималната хиперплоскост, която разделя данните на различни класове с максимален марж.

Основни концепции
Хиперплоскост (Hyperplane):
Хиперплоскостта е границата, която разделя данните на два или повече класа. За двумерно пространство тя е права линия, за тримерно е равнина, а за по-високи размерности е хиперплоскост.

Марж (Margin):
Това е разстоянието между хиперплоскостта и най-близките точки от всеки клас. Целта на SVM е да максимизира този марж, за да осигури най-добрата разделителна способност.

Support Vectors:
Това са точките, които са най-близо до хиперплоскостта и определят нейното положение. Те са критични за модела, тъй като ако бъдат премахнати, хиперплоскостта ще се промени.

Как работи SVM?
Линеен SVM:
Алгоритъмът намира праволинейна хиперплоскост, която разделя данните.
Подходящ за линейно разделими данни.
Нелинеен SVM:
Когато данните не могат да се разделят линейно, се използва kernel trick.
Kernel trick трансформира данните в пространство с по-висока размерност, където те стават линейно разделими.

Kernel Trick
Kernel Trick е техника, използвана в алгоритми като Support Vector Machines (SVM), за да се справят с нелинейни зависимости в данните. Основната идея е:
Прехвърляне в пространство с по-висока размерност: Вместо да работим в оригиналното (често линейно) пространство на характеристиките, Kernel Trick позволява данните да бъдат преобразувани в пространство с по-висока размерност, където могат да бъдат по-лесно разделени.
Без директно изчисляване: Вместо експлицитно да се изчисляват координатите на новото пространство, Kernel Trick изчислява само скаларните произведения между точките в това пространство. Това значително спестява време и ресурси.
Пример:
Оригиналните данни в 2D пространство може да са нелинейно разделими.
Kernel Trick ги прехвърля в 3D пространство, където стават линейно разделими.
Популярни ядра (kernels):
Linear Kernel: Работи без прехвърляне на данните в по-висока размерност.
Polynomial Kernel: Използва полиномиална трансформация на данните.
Radial Basis Function (RBF): Моделира сложни нелинейни зависимости.
Sigmoid Kernel: Прилича на сигмоидната функция, използвана в невронните мрежи.

Значение на параметрите 𝐶 и 𝛾 (gamma)
C (Regularization Parameter)
Контролира разменната стойност между:
Максимизиране на маржовете (margins): Стремежът да се създаде модел, който разделя класовете възможно най-добре.
Минимизиране на грешките (classification error): Позволява на модела да игнорира шум в данните.
Как влияе:
Голямо 𝐶: Стреми се да класифицира всички точки правилно, но може да доведе до overfitting.
Малко 𝐶: Позволява повече грешки, което прави модела по-обобщаващ (underfitting).
𝛾 (Gamma) в RBF и други нелинейни ядра
Определя влиянието на отделните точки:
Как всяка точка от обучаващите данни влияе върху модела.
Как влияе:
Голямо 𝛾: Всяка точка има голямо влияние, което може да направи модела много сложен и чувствителен към шум (overfitting).
Малко 𝛾: Всяка точка влияе върху по-голяма област, което може да доведе до по-груб модел (underfitting).

Пример с интуиция
Представи си задача за разделяне на два класа (сини и червени точки):
𝐶: Контролира дали искаме да избегнем грешки (строга граница) или сме готови да толерираме грешки (по-гъвкава граница).
𝛾: Определя "обхвата" на влияние на всяка точка върху хиперплоскостта. Голямо 
𝛾 означава, че всяка точка силно "изтегля" границата към себе си.
Визуална демонстрация
Голямо 𝐶 и 𝛾: Моделът ще създаде сложна граница, която почти перфектно разделя точките, но е податлива на шум.
Малко 𝐶 и 𝛾: Моделът ще има по-проста граница, но може да пропусне по-сложни зависимости.


4.K-Nearest Neighbors (KNN)
Класифицира обекти въз основа на техните най-близки съседи.
Как да изберем броя на съседите (k)?
Малко 𝑘:
Моделът става чувствителен към шум.
Може да доведе до overfitting, защото взема предвид само най-близките точки.
Пример: Ако k=1, алгоритъмът просто копира най-близкия съсед, дори и той да е шум.
Голямо 𝑘:
Увеличава гладкостта на решението.
Може да доведе до underfitting, защото включва прекалено много точки, които може да не са свързани с локалната структура.
Как да изберем 𝑘:
Пробваме различни стойности чрез кръстосана проверка (cross-validation).
Най-често използвани стойности на 𝑘: малки нечетни числа (k=3,5,7), за да се избегнат равенства при класификация.
2. Как влияе мащабирането на данните върху KNN?
Защо е важно?
KNN използва евклидово разстояние (или друго метрика) за измерване на близостта между точки.
Ако характеристиките имат различни мащаби (например: една е в метри, друга в милиони долари), тези с по-големи стойности доминират разстоянието.
Решение:
Нормализация:
Преобразуване на характеристиките, така че те да са между 0 и 1.
Формула: (x−min(x))/(max(x)−min(x)).
Стандартизация:
Преобразуване на характеристиките с нулева средна стойност и единична стандартна девиация.
Формула: (x−μ)/σ, където 𝜇 е средната стойност, а 𝜎 е стандартното отклонение.
Пример:
Ако не мащабираме данните, характеристика като "години" може да бъде игнорирана спрямо "заплата в хиляди".

Слабости на KNN
Бавен за големи набори от данни:
Изисква изчисляване на разстояния за всяка точка при предсказване.
Решение: KD-Tree или Ball-Tree за оптимизация.
Чувствителен към шум и аномалии:
Пример: Аномална точка може да промени класификацията за съседите си.
Работи с всички характеристики:
Не взема предвид кои характеристики са релевантни за задачата.


#### Ансамблови методи

1.Gradient Boosting (например XGBoost, LightGBM)
Подходящи за състезания по данни (като Kaggle).

Каква е разликата между boosting и bagging?
Boosting: Построява модели последователно, като всеки нов модел коригира грешките на предишния.
Bagging: Построява модели успоредно и усреднява резултатите.

Какво е значението на learning rate?
Контролира стъпките на оптимизацията. Малък learning rate = по-точна, но по-бавна оптимизация.


2.AdaBoost
Алгоритъм за подобряване на производителността на слаби ученици (weak learners).
Може да те питат за: как работи теглото на примерите, приложими случаи.

##########################################################################################

Разликата между Decision Trees (решаващи дървета) и Random Forest (случайна гора) се основава на тяхната структура, метод на обучение и поведение при различни задачи. Ето основните разлики:

1. Основна структура
Decision Tree:
Съдържа само едно дърво, което разделя данните на базата на правила.
Работи като йерархична структура, в която всеки възел разделя данните според определена характеристика.
Random Forest:
Използва множество решаващи дървета, които работят заедно като ансамблов модел.
Крайният резултат е комбинация от предвижданията на всички дървета (средно аритметично за регресия или гласуване за класификация).
2. Точност и стабилност
Decision Tree:
Може лесно да се адаптира към обучителните данни, но това го прави податливо на overfitting.
Чувствително е към малки промени в данните, което може да доведе до различни структури на дървото.
Random Forest:
Намалява проблема с overfitting, защото обобщава резултатите от много дървета.
По-стабилен и по-точен при работа с шумни и сложни данни.
3. Случайност (Randomness)
Decision Tree:
Използва всички налични данни и характеристики, за да разделя възлите.
Random Forest:
Добавя случайност чрез:
Случайни поднабори от данните (Bagging).
Случайни поднабори от характеристиките при разделяне на възли.
4. Скорост
Decision Tree:
По-бързо за обучение и предвиждане, тъй като използва само едно дърво.
Random Forest:
По-бавно за обучение и предвиждане, защото тренира множество дървета и комбинира резултатите.
5. Интерпретация
Decision Tree:
Лесно интерпретируемо – можеш да проследиш решенията, взети от модела, за конкретно предвиждане.
Random Forest:
Трудно интерпретируемо, защото комбинира много дървета, всяко със свои собствени правила.
6. Приложение
Decision Tree:
Добър избор за задачи с малки или средно големи данни, където точността не е критична.
Random Forest:
Подходящ за сложни задачи, където точността и стабилността са важни, като кредитен риск, прогнозиране на отлив (churn prediction) и откриване на измами.
7. Изчисляване на важността на характеристиките
Decision Tree:
Показва кои характеристики са най-важни в конкретното дърво.
Random Forest:
Изчислява общата важност на характеристиките, базирана на приноса им към всички дървета.
Пример
Decision Tree:
Моделът може да направи грешни предвиждания, ако структурата на данните е твърде сложна или шумна.

Random Forest:
Комбинира множество дървета, като заглушава грешките на индивидуалните дървета и предоставя по-точни предвиждания.

######################################################################################################
Как да изберем метриката за разстояние?

Евклидово разстояние: Най-често използвано.
Подходящо за данни в еднаква мащабна скала.

Манхатън разстояние:
Използва сума от абсолютните разлики между координатите.
Подходящо, ако данните са разпръснати по осите.

Косинусова подобност:
Подходяща за текстови данни или вектори с голяма размерност.

Махааланобис разстояние:
Взема предвид корелацията между характеристиките.
Подходящо за данни с различна разпределеност.

#####################################################################################################

Ансамбловите методи (ensemble methods) са техники в машинното обучение, при които се комбинират множество модели, за да се постигне по-добра обща производителност, отколкото всеки от отделните модели би постигнал самостоятелно. Основната идея е, че комбинирайки няколко по-слаби модела (weak learners), можем да създадем по-силен и по-устойчив модел (strong learner).

Как работят ансамбловите методи?
Ансамбловите методи разчитат на следното:
Диверсификация: Използването на различни модели или различни версии на един и същи модел води до по-малко пристрастия и по-голяма устойчивост.
Комбинация: Резултатите от всички модели се обединяват чрез агрегиране (например осредняване или гласуване).

Основни видове ансамблови методи

Bagging (Bootstrap Aggregating):
Основна идея: Намалява вариацията в модела чрез комбиниране на резултатите от няколко независими модела, обучени върху различни подмножества от данните.
Пример: Random Forest.
Random Forest обучава множество дървета за вземане на решения (decision trees), като всеки от тях работи с различна случайна подмножество от данните и характеристиките.
Намалява overfitting, защото обединява резултатите чрез усредняване (за регресия) или гласуване (за класификация).

Boosting:
Основна идея: Намалява грешките в модела чрез последователно обучение на модели, където всеки следващ модел коригира грешките на предходния.
Примери: AdaBoost, Gradient Boosting, XGBoost.
Тези алгоритми поставят по-голяма тежест на грешно класифицираните или прогнозирани точки, за да подобрят точността.

Stacking (Stacked Generalization):
Основна идея: Комбинира различни модели чрез метамодел, който се обучава да предсказва крайните резултати въз основа на изходите на базовите модели.
Пример: Използване на Logistic Regression или Neural Network като метамодел, който обединява резултатите от Random Forest, SVM, и други.

Защо ансамблите намаляват overfitting?
Диверсификация на грешките: Различните модели правят различни грешки, които се неутрализират при обединяването.
Намаляване на вариацията: Обединяването на резултатите от няколко модела води до по-стабилни и надеждни предсказания.
Случайност: В Random Forest, например, случайното избиране на данни и характеристики при обучение на дърветата намалява зависимостта от конкретен набор данни.

