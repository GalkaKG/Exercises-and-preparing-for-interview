Разлика между Supervised, Unsupervised и Reinforcement Learning
Тип ML	Описание	Примери
Supervised Learning	Моделът се обучава на основа на данни с етикети (input-output). Целта е да се предвиди изходна стойност за нови данни.	- Кредитен риск (предсказване на "default" или "non-default").
- Прогнозиране на отлив.
Unsupervised Learning	Моделът намира скрити структури в данните, които нямат етикети (само input).	- Групиране на клиенти (segmentation).
- Намаляване на размерността (PCA).
Reinforcement Learning	Моделът се обучава чрез взаимодействие със средата и получава награди/наказания за своите действия.	- Оптимизация на кредитно портфолио.
- Търговски стратегии.


#############################################
Алгоритми за Supervised Learning (с надзор)

##### Регресия

1.Линейна регресия (Linear Regression):
Използва се за прогнозиране на непрекъснати стойности (например цена на жилище).

Какво представлява хипотезата в линейната регресия?
Хипотезата е линейна функция, която моделира връзката между входните променливи (features) и изходната променлива (target):

Как се оценява грешката (MSE/MAE)?
MSE (Mean Squared Error): Средната стойност на квадрата на грешките:
MAE (Mean Absolute Error): Средната абсолютна стойност на грешките:

Как се интерпретират коефициентите?
Коефициентите показват как промяната на съответната характеристика с единица влияе на целевата променлива, при фиксиране на останалите характеристики.



2.Логистична регресия (Logistic Regression):
Използва се за класификация (например бинарна класификация - одобрение на кредит).


Каква е функцията, използвана за класификация?
Логистичната регресия използва сигмоидна функция за прогнозиране на вероятности

Как интерпретираме изхода?
Изходът е вероятност за принадлежност към даден клас (обикновено бинарен). Ако вероятността е над 0.5, се приема клас 1, иначе клас 0.


##### Класификация

1.Decision Trees (Решаващи дървета)

Подходящи за разбираеми и лесно интерпретируеми модели.

Какво представляват критериите за разделяне?
Gini Index: Мярка за чистота на клъстерите. По-малка стойност = по-хомогенен клъстер.
Entropy (Information Gain): Мярка за намаляване на несигурността при разделяне.

Какви са предимствата и недостатъците?
Предимства: Лесни за разбиране и интерпретация, подходящи за категориални и числови данни.
Недостатъци: Могат да overfit-нат, ако са твърде дълбоки.

2.Random Forest (Случайни гори)
Съставен алгоритъм, който комбинира множество решаващи дървета.

Как намалява overfitting?
Комбинира множество независими дървета и усреднява резултатите, което намалява пренасищането с данни.

Как се определя значимостта на характеристиките?
Значимостта се изчислява въз основа на това колко пъти и колко ефективно дадена характеристика участва в разделянето на данните.

Random Forest е мощен алгоритъм за машинно обучение, който използва концепцията за ансамбли. Основните характеристики на Random Forest включват:
1. Ансамблов метод
Random Forest комбинира множество решаващи дървета (decision trees) в един ансамблов модел.
Крайният резултат се базира на:
Средна стойност на предвижданията за регресионни задачи.
Гласуване с мнозинство за класификационни задачи.
2. Случайност (Randomness)
Случайност в избора на данни:
Използва техниката Bagging (Bootstrap Aggregation), като всяко дърво се обучава на случайно избран поднабор от данните.
Случайност в избора на характеристики (features):
При всяко разделяне на възел в дървото, Random Forest избира само поднабор от характеристиките вместо всички.
3. Редуциране на Overfitting
Чрез комбиниране на много дървета и използване на случайност в обучението, Random Forest намалява риска от overfitting спрямо единичните решаващи дървета.
4. Нелинейни зависимости
Random Forest може да улавя сложни и нелинейни зависимости между характеристиките, тъй като дърветата разделят пространството на данните на сложни области.
5. Оценка на важността на характеристиките
Random Forest може да изчисли важността на характеристиките, като измерва колко често дадена характеристика се използва за разделяне на възел в дърветата.
6. Вътрешна валидирация с OOB (Out-of-Bag Error)
Моделът използва данни, които не са били избрани в случайния поднабор (out-of-bag data), за да оцени точността си, без да е нужно отделно тестово множество.
7. Устойчивост към шум и липсващи данни
Random Forest е устойчив на:
Шум в данните, защото ансамбловият подход заглушава влиянието на грешните предвиждания.
Липсващи стойности, тъй като разделянията в дърветата не зависят задължително от всички характеристики.
8. Поддръжка на висока дименсионалност
Random Forest работи добре с голям брой характеристики, като избира поднабори от тях за всяко дърво.
Random Forest може да се използва за:
Класификационни задачи (например предвиждане на класове).
Регресионни задачи (например предвиждане на числени стойности).
10. Контролируеми хиперпараметри
Random Forest има няколко важни хиперпараметри, които позволяват настройка на модела според задачата:
n_estimators: Броят на дърветата в гората.
max_depth: Максималната дълбочина на всяко дърво.
max_features: Максималният брой характеристики, които да се използват за разделяне.
min_samples_split: Минималният брой примери, необходими за разделяне на възел.
min_samples_leaf: Минималният брой примери в листо.
Недостатъци на Random Forest
По-голямо време за обучение спрямо единични дървета.
Трудна интерпретация на модела, тъй като комбинира множество дървета.
Неефективен при екстремно големи данни, когато няма достатъчно ресурси.


